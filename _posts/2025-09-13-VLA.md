---
layout: post # 指定文章佈局，通常是 post
title: VLA
date: 2025-09-13 10:00:00 +0800 # 發表日期和時間 (請根據您當前的時區調整 +0800 代表 UTC+8)
categories: [Lecture] # 文章分類，您可以自訂
tags: [GenAI] # 文章標籤，您可以自訂
description: Vision-Language-Action model
mathjax: false # 如果這篇文章不需要顯示數學公式，請設false
comments: false # 如果這篇文章需要啟用評論，請設為 true

---
## Vision-Language-Action models (VLA)

**Paper**: [Vision-Language-Action Models: Concepts, Progress, Applications and Challenges](https://arxiv.org/html/2505.04769v1)<br>
<p><img width="50%" height="50%" src="https://arxiv.org/html/2505.04769v1/x1.png"></p>

<p><img width="50%" height="50%" src="https://arxiv.org/html/2505.04769v1/x3.png"></p>
<p><img width="50%" height="50%" src="https://arxiv.org/html/2505.04769v1/x4.png"></p>

---
### [Open X-Embodiment](https://github.com/google-deepmind/open_x_embodiment)
#### Paper: [Open X-Embodiment: Robotic Learning Datasets and RT-X Models](https://arxiv.org/html/2310.08864v9)<br>

<p><img width="50%" height="50%" src="https://github.com/google-deepmind/open_x_embodiment/raw/main/imgs/teaser.png"></p>

---
### [Robotics Transformer (RT-1)](https://research.google/blog/rt-1-robotics-transformer-for-real-world-control-at-scale/)
![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj11ho9tm4Td7ByTigAgDxFWsxbsZ6tQsAng3AtwuufHRuoLaLOV9YN7FUMyyAhefzuFOVCrbwTLsEaRYidOOToS__KRrotot-6aBxTliZxYz-B2jiJG-4myq5NB3vRKaY86nr5y1-13dBv_H_XyfnDijphCM3UBalczim0PeGJ63Z0Ok6k9zvKQ2D55A/s16000/image1.png)


---
### [BridgeData V2](https://rail-berkeley.github.io/bridgedata/)

#### Paper: [BridgeData V2: A Dataset for Robot Learning at Scale](https://arxiv.org/html/2308.12952v3)

#### Code: [https://github.com/rail-berkeley/bridge_data_v2](https://github.com/rail-berkeley/bridge_data_v2)

---
### [OpenVLA](https://github.com/openvla/openvla)
**Paper**: [OpenVLA: An Open-Source Vision-Language-Action Model](https://arxiv.org/abs/2406.09246)<br>
<iframe width="474" height="434" src="https://www.youtube.com/embed/0Cld-U6EQt4" title="Introducing X-Series 6DOF Arms" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

#### Installation
```
!git clone https://github.com/openvla/openvla
%cd openvla

!pip install -r requirements-min.txt
```

For example, to load openvla-7b for zero-shot instruction following in the BridgeData V2 environments with a WidowX robot:<br>
```
# Install minimal dependencies (`torch`, `transformers`, `timm`, `tokenizers`, ...)
# > pip install -r https://raw.githubusercontent.com/openvla/openvla/main/requirements-min.txt
from transformers import AutoModelForVision2Seq, AutoProcessor
from PIL import Image

import torch

# Load Processor & VLA
processor = AutoProcessor.from_pretrained("openvla/openvla-7b", trust_remote_code=True)
vla = AutoModelForVision2Seq.from_pretrained(
    "openvla/openvla-7b", 
    attn_implementation="flash_attention_2",  # [Optional] Requires `flash_attn`
    torch_dtype=torch.bfloat16, 
    low_cpu_mem_usage=True, 
    trust_remote_code=True
).to("cuda:0")

# Grab image input & format prompt
image: Image.Image = get_from_camera(...)
prompt = "In: What action should the robot take to {<INSTRUCTION>}?\nOut:"

# Predict Action (7-DoF; un-normalize for BridgeData V2)
inputs = processor(prompt, image).to("cuda:0", dtype=torch.bfloat16)
action = vla.predict_action(**inputs, unnorm_key="bridge_orig", do_sample=False)

# Execute...
robot.act(action, ...)
```

---
### OpenVLA-OFT
**Paper**: [Fine-Tuning Vision-Language-Action Models: Optimizing Speed and Success](https://arxiv.org/abs/2502.19645)<br>
**Code**: [https://github.com/moojink/openvla-oft](https://github.com/moojink/openvla-oft)<br>
<iframe width="576" height="268" src="https://www.youtube.com/embed/T3Zkkr_NTSA" title="Optimized Fine-Tuning Recipe for VLAs (OpenVLA-OFT) Summary Video" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

---
### [SmolVLA](https://huggingface.co/blog/smolvla)
**Model**: [lerobot/smolvla_base](https://huggingface.co/lerobot/smolvla_base)<br>
**Paper**: [SmolVLA: A Vision-Language-Action Model for Affordable and Efficient Robotics](https://arxiv.org/abs/2506.01844)<br>
![](https://arxiv.org/html/2506.01844v1/x16.png)

#### [SO-101 robot](https://huggingface.co/docs/lerobot/en/so101)
<iframe width="514" height="289" src="https://www.youtube.com/embed/LxluHcx0BUA" title="AI Robot Arm - LeRobot SO-101 - First Try and Burned It #ai #robotics #robot" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

#### [SO-ARM101 AI 機器手臂PRO套件 for LeRobot](https://www.icshop.com.tw/products/368040500233)
![](https://shoplineimg.com/6486dbe2afaddb00694ea79f/686c83dbdae506001031a682/800x.webp?source_format=jpg)


<br>
<br>

*This site was last updated {{ site.time | date: "%B %d, %Y" }}.*

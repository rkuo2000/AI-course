---
layout: post # 指定文章佈局，通常是 post
title: VLM/MLLMs
date: 2025-09-14 08:00:00 +0800 # 發表日期和時間 (請根據您當前的時區調整 +0800 代表 UTC+8)
categories: [Lecture] # 文章分類，您可以自訂
tags: [GenAI] # 文章標籤，您可以自訂
description: Vision Language Model / Multimodal Large Language Model
mathjax: false # 如果這篇文章不需要顯示數學公式，請設false
comments: false # 如果這篇文章需要啟用評論，請設為 true

---
## MLLM - Multimodal Large Language Model
**Paper:** [A Survey on Multimodal Large Language Models](https://arxiv.org/abs/2306.13549)<br>
![](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/raw/main/images/timeline.jpg)

### [MLLM papers](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models)

---
## VLM - Vision Language Model
### [Guide to Vision-Language Models (VLMs)](https://encord.com/blog/vision-language-models-guide)
**[LLM in Vision papers](https://github.com/DirtyHarryLYL/LLM-in-Vision)**<br>

### Contrastive Learning
**CLIP architecture**<br>
![](https://images.prismic.io/encord/04311c42-2635-40fb-9187-2847b87224a7_image9.png?auto=compress,format)

---
### PrefixLM
**SimVLM architecture**<br>
![](https://images.prismic.io/encord/80a849e3-8867-4414-bde0-da76df2314de_image3.png?auto=compress,format)

**VirTex architecture**<br>
![](https://images.prismic.io/encord/39a495c5-b563-4253-8722-c58dda94eeb3_image2.png?auto=compress,format)

**Frozen architecture**<br>
![](https://images.prismic.io/encord/65b4b1ac-6af0-41e2-9ac7-e7cdbd67a589_image10.png?auto=compress,format)

**Flamingo architecture**<br>
![](https://images.prismic.io/encord/b94edb70-7caf-4944-9836-2d58dad8b91a_image4.png?auto=compress,format)

---
### Multimodal Fusing with Cross-Attention
**VisualGPT architecture**<br>
![](https://images.prismic.io/encord/90aecd3e-cb7d-4df9-b0df-a64c1a9f9a9d_image6.png?auto=compress,format)

---
### Masked-language Modeling (MLM) & Image-Text Matching (ITM)
**VisualBERT architecture**<br>
![](https://images.prismic.io/encord/9e2fdf0f-24ca-4bf6-9df2-712da43be018_image12.png?auto=compress,format)

---
## Multimodal AI
[Multimodal AI: A Guide to Open-Source Vision Language Models](https://www.bentoml.com/blog/multimodal-ai-a-guide-to-open-source-vision-language-models)<br>

**Arxiv**: [MM-LLMs: Recent Advances in MultiModal Large Language Models](https://arxiv.org/html/2401.13601v1)<br>
![](https://arxiv.org/html/2401.13601v1/x1.png)

**The general model architecture of MM-LLMs**<br>
![](https://arxiv.org/html/2401.13601v1/x2.png)

#### Gemma 3
Available in 1B, 4B, 12B, and 27B sizes. With a 128K-token context window (32K for 1B).<br>

#### GLM-4.1V-Thinking
an open-source VLM developed by Z.ai. With just 9 billion parameters and a 64K-token context window.<br>

#### [Llama3.2 Vision](https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/)<br>

#### NVLM 1.0
a family of multimodal LLMs developed by NVIDIA<br>

#### Molmo
Available in 1B, 7B, and 72B parameters<br>

#### Qwen2.5-VL
Available in 3B, 7B, 32B and 72B parameter sizes and offers strong multimodal performance across vision, language, document parsing, and long video understanding.<br>

#### [Pixtral](https://arxiv.org/abs/2410.07073) 

---
### [PaLM-E](https://palm-e.github.io/)
**Arxiv**: [PaLM-E: An Embodied Multimodal Language Model](https://arxiv.org/abs/2303.03378)<br>
**Github**: [https://github.com/kyegomez/PALM-E](https://github.com/kyegomez/PALM-E)<br>
![](https://github.com/kyegomez/PALM-E/raw/main/image6.png)

---
### [LLaVA](https://llava-vl.github.io/)
**Arxiv**: [Visual Instruction Tuning](https://arxiv.org/abs/2304.08485)<br>
**Arxiv**: [Improved Baselines with Visual Instruction Tuning](https://arxiv.org/abs/2310.03744)<br>
**Github**: [https://github.com/haotian-liu/LLaVA](https://github.com/haotian-liu/LLaVA)<br>
![](https://github.com/rkuo2000/GenAI/blob/main/assets/LLaVA_Gradio_Server_UI.png?raw=true)

---
### LLaVA-Med
**Arxiv**: [LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day](https://arxiv.org/abs/2306.00890)<br>
**Github**: [https://github.com/microsoft/LLaVA-Med](https://github.com/microsoft/LLaVA-Med)<br>

---
### Qwen-VL
**Arxiv**: [Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond](https://arxiv.org/abs/2308.12966)<br>
**Github**: [https://github.com/QwenLM/Qwen-VL](https://github.com/QwenLM/Qwen-VL)<br>

---
### [LLaVA-Plus](https://llava-vl.github.io/llava-plus/)
**Arxiv**: [LLaVA-Plus: Learning to Use Tools for Creating Multimodal Agents](https://arxiv.org/abs/2311.05437)<br>
**Github**: [https://github.com/LLaVA-VL/LLaVA-Plus-Codebase](https://github.com/LLaVA-VL/LLaVA-Plus-Codebase)<br>
![](https://llava-vl.github.io/llava-plus/images/llava-plus-hero.png)
![](https://llava-vl.github.io/llava-plus/images/llava-plus-example.png)

---
### GPT4-V
**Arxiv**: [Assessing GPT4-V on Structured Reasoning Tasks](https://arxiv.org/abs/2312.11524)<br>

---
### Gemini
**Arxiv**: [Gemini: A Family of Highly Capable Multimodal Models](https://arxiv.org/abs/2312.11805)<br>
![](https://github.com/rkuo2000/AI-course/blob/main/images/Gemini.png?raw=true)

---
### Yi-VL-34B
**HuggineFace**: [01-ai/Yi-VL-34B](https://huggingface.co/01-ai/Yi-VL-34B)<br>

---
### FuYu-8B
**Blog**: [Fuyu-8B: A Multimodal Architecture for AI Agents](https://www.adept.ai/blog/fuyu-8b)<br>
![](https://www.adept.ai/images/blog/fuyu-8b/architecture.png)
![](https://github.com/rkuo2000/AI-course/blob/main/images/VLMs_benchmark.png?raw=true)



---
### [LLaVA-NeXT](https://llava-vl.github.io/blog/2024-01-30-llava-next/)
LLaVA-NeXT: Improved reasoning, OCR, and world knowledge<br>
Compared with LLaVA-1.5, LLaVA-NeXT has several improvements:<br>
* Increasing the input image resolution to 4x more pixels. This allows it to grasp more visual details. It supports three aspect ratios, up to 672x672, 336x1344, 1344x336 resolution.
* Better visual reasoning and OCR capability with an improved visual instruction tuning data mixture.
* Better visual conversation for more scenarios, covering different applications. Better world knowledge and logical reasoning.
* Efficient deployment and inference with SGLang.

---
### [Florence-2](microsoft/Florence-2-large)
**model:** [microsoft/Florence-2-large](https://huggingface.co/microsoft/Florence-2-large)<br>
**Paper:** [Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks](https://arxiv.org/abs/2311.06242)<br>
**Blog:** [Florence-2: Advancing Multiple Vision Tasks with a Single VLM Model](https://towardsdatascience.com/florence-2-mastering-multiple-vision-tasks-with-a-single-vlm-model-435d251976d0)<br>
![](https://miro.medium.com/v2/resize:fit:720/format:webp/1*AgN5ojBuiXe1GQyfWkHwRQ.png)

---
### VILA 
**Paper:** [VILA: On Pre-training for Visual Language Models](https://arxiv.org/abs/2312.07533)<br>
**Code:** [https://github.com/Efficient-Large-Model/VILA](https://github.com/Efficient-Large-Model/VILA)<br>
#### VILA on Jetson Orin
<video width="779" height="540" controls><source src="https://github.com/rkuo2000/AI-course/raw/main/images/VILAonJetsonOrin.mp4" type="video/mp4"></video>

---
### [VLFeedback and Silkie](https://vlf-silkie.github.io/)
**Paper:** [Silkie: Preference Distillation for Large Visual Language Models](https://arxiv.org/abs/2312.10665)<br>
**Code:** [https://github.com/vlf-silkie/VLFeedback](https://github.com/vlf-silkie/VLFeedback)<br>

---
### MobileVLM
**Paper:** [MobileVLM V2: Faster and Stronger Baseline for Vision Language Model](https://arxiv.org/abs/2402.03766)<br>
**Code:** [https://github.com/Meituan-AutoML/MobileVLM](https://github.com/Meituan-AutoML/MobileVLM)<br>

---
### [MyVLM](https://snap-research.github.io/MyVLM/)
**Paper:** [MyVLM: Personalizing VLMs for User-Specific Queries](https://arxiv.org/abs/2403.14599)<br>
**Code:** [https://github.com/snap-research/MyVLM](https://github.com/snap-research/MyVLM)<br>
![](https://github.com/snap-research/MyVLM/blob/master/docs/method.jpg?raw=true)
![](https://github.com/snap-research/MyVLM/raw/master/docs/teaser.jpg)

---
### [Reka Core](https://reka-ai)
**Paper:** [Reka Core, Flash, and Edge: A Series of Powerful
Multimodal Language Models](https://publications.reka.ai/reka-core-tech-report.pdf)<br>
![](https://the-decoder.com/wp-content/uploads/2024/04/reka_architecture.png)
![](https://images.squarespace-cdn.com/content/v1/66118bc053ae495c0021e80f/811fc96a-79e1-42d2-95a6-efc0b5e7e57d/reka+core%28blog%29+%283%29.jpg?format=1500w)

---
### InternLM-XComposer
**Code:** [https://github.com/InternLM/InternLM-XComposer](https://github.com/InternLM/InternLM-XComposer)<br>
**InternLM-XComposer2-4KHD** could further understand 4K Resolution images.<br>
![](https://github.com/InternLM/InternLM-XComposer/raw/main/assets/4khd_radar.png)


---
### [MiniCPM-V](https://github.com/OpenBMB/MiniCPM-V)
**HuggingFace**: [openbmb/MiniCPM-Llama3-V-2_5-int4](https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5-int4)<br>
**Arxiv**: [MiniCPM-V: A GPT-4V Level MLLM on Your Phone](https://arxiv.org/abs/2408.01800)<br>
![](https://github.com/OpenBMB/MiniCPM-V/raw/main/assets/MiniCPM-Llama3-V-2.5-peformance.png)

---
### SoM
**Arxiv**: [Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V](https://arxiv.org/abs/2310.11441)<br>
**Github**: [https://github.com/microsoft/SoM](https://github.com/microsoft/SoM)<br>
![](https://github.com/microsoft/SoM/blob/main/assets/method2_xyz.png?raw=true)

---
### [Gemini-1.5](https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/)
**Arxiv**: ]Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context](https://arxiv.org/abs/2403.05530)<br>

---
### SoM-LLaVA
**Arxiv**: [List Items One by One: A New Data Source and Learning Paradigm for Multimodal LLMs](https://arxiv.org/abs/2404.16375)<br>
**Github**: [https://github.com/zzxslp/SoM-LLaVA](https://github.com/zzxslp/SoM-LLaVA)<br>
![](https://github.com/zzxslp/SoM-LLaVA/blob/main/examples/case1.png?raw=true)
![](https://github.com/zzxslp/SoM-LLaVA/blob/main/examples/case2.png?raw=true)

---
### Phi-3 Vision
**HuggineFace**: [microsoft/Phi-3-vision-128k-instruct](https://huggingface.co/microsoft/Phi-3-vision-128k-instruct)<br>
**Arxiv**: [Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone](https://arxiv.org/html/2404.14219v1)<br>
Phi-3-vision is a **4.2B** parameter multimodal model with language and vision capabilities.<br>

---
### EVE
**Arxiv**: [Unveiling Encoder-Free Vision-Language Models](https://arxiv.org/html/2406.11832v2)<br>
![](https://arxiv.org/html/2406.11832v2/x2.png)

---
### [Paligemma](https://huggingface.co/blog/paligemma)
**mode:** [google/paligemma-3b-pt-224](https://huggingface.co/google/paligemma-3b-pt-224)<br>
**Paper:** [PaliGemma: A versatile 3B VLM for transfer](https://arxiv.org/abs/2407.07726)<br>
![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/paligemma/paligemma_arch.png)

---
### [CogVLM2](https://github.com/THUDM/CogVLM2)
**Paper:** [CogVLM2: Visual Language Models for Image and Video Understanding](https://arxiv.org/abs/2408.16500)<br>
**[Demo](http://cogvlm2-online.cogviewai.cn:7868/)** <br>

---
### LongLLaVA
**Blog**: [LongLLaVA: Revolutionizing Multi-Modal AI with Hybrid Architecture](https://blog.gopenai.com/revolutionizing-multi-modal-ai-how-longllava-efficiently-handles-1000-images-with-hybrid-59236bb06609)<br>
**Arxiv**: [LongLLaVA: Scaling Multi-modal LLMs to 1000 Images Efficiently via Hybrid Architecture](https://arxiv.org/abs/2409.02889)<br>
![](https://miro.medium.com/v2/resize:fit:720/format:webp/1*F5fk272n2Qsb73l1JZd7WA.jpeg)
**Github**: [https://github.com/FreedomIntelligence/LongLLaVA](https://github.com/FreedomIntelligence/LongLLaVA)<br>

---
### Phi-3.5-vision
**HuggineFace**: [microsoft/Phi-3.5-vision-instruct](https://huggingface.co/microsoft/Phi-3.5-vision-instruct)<br>

---
### Pixtral
**HuggingFace**: [mistralai/Pixtral-12B-2409](https://huggingface.co/mistralai/Pixtral-12B-2409)<br>
**Arxiv**: [Pixetral 12B](https://arxiv.org/abs/2410.07073)<br>

---
### [Qwen-Audio](https://qwen-audio.github.io/Qwen-Audio/)
**Paper:** [Qwen-Audio: Advancing Universal Audio Understanding via Unified Large-Scale Audio-Language Models](https://arxiv.org/abs/2311.07919)<br>
**Code:** [https://github.com/QwenLM/Qwen-Audio](https://github.com/QwenLM/Qwen-Audio)<br>
![](https://qwen-audio.github.io/Qwen-Audio/static/images/framework.png)
![](https://qwen-audio.github.io/Qwen-Audio/static/audio/radar_new.png)

---
### Octopus
**Arxiv**: [Octopus: Embodied Vision-Language Programmer from Environmental Feedback](https://arxiv.org/abs/2310.08588)<br>
**Github**: [https://github.com/dongyh20/Octopus](https://github.com/dongyh20/Octopus)<br>
<iframe width="942" height="530" src="https://www.youtube.com/embed/tmSNw2XonxI" title="Introducing Project Octopus" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

---
### [VLM-R1](https://github.com/om-ai-lab/VLM-R1)
VLM-R1: A stable and generalizable R1-style Large Vision-Language Model<br>
![](https://github.com/om-ai-lab/VLM-R1/raw/main/assets/performance.png)

---
### [NaVid](https://pku-epic.github.io/NaVid/)
**Arxiv**: [NaVid: Video-based VLM Plans the Next Step for Vision-and-Language Navigation](https://arxiv.org/abs/2402.15852)<br>
![](https://pku-epic.github.io/NaVid/static/images/method.png)

<br>
<br>

*This site was last updated {{ site.time | date: "%B %d, %Y" }}.*



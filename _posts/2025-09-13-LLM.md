---
layout: post # 指定文章佈局，通常是 post
title: LLM
date: 2025-09-13 08:00:00 +0800 # 發表日期和時間 (請根據您當前的時區調整 +0800 代表 UTC+8)
categories: [Lecture] # 文章分類，您可以自訂
tags: [GenAI] # 文章標籤，您可以自訂
description: Introduction to LLM
mathjax: false # 如果這篇文章不需要顯示數學公式，請設false
comments: false # 如果這篇文章需要啟用評論，請設為 true

---
## History of LLMs
[A Survey of Large Language Models](https://arxiv.org/abs/2303.18223)<br>

### LLM Timeline
![](https://www.researchgate.net/publication/384694535/figure/fig3/AS:11431281431591779@1746800961963/The-timeline-of-the-development-of-LLMs-from-2018-to-2024-June-showcasing-key.tif)
![](https://www.researchgate.net/publication/393983430/figure/fig3/AS:11431281562035097@1753645489558/Timeline-of-major-LLM-releases-2023-Early-2025-showing-the-rapid-evolution-of-LLMs.png)

---
### 計算記憶體的成長與Transformer大小的關係
**Paper**: [AI and Memory Wall](https://arxiv.org/html/2403.14123v1)<br>
![](https://blocksandfiles.com/wp-content/uploads/2025/01/UNifabriX-Memory-wall-chart-with-PCIe-and-CXL.jpg)

---
### Scaling Law
我們可以用模型大小、Dataset大小、總計算量，來預測模型最終能力。（通常以相對簡單的函數型態, ex: Linear relationship）<br>
[GPT-4 Technical Report. OpenAI. 2023](https://arxiv.org/pdf/2303.08774.pdf)<br>

**Blog:** [【LLM 10大觀念-1】Scaling Law](https://axk51013.medium.com/llm%E5%B0%88%E6%AC%84-%E8%BF%8E%E6%8E%A52024%E5%B9%B4-10%E5%80%8B%E5%BF%85%E9%A0%88%E8%A6%81%E6%90%9E%E6%87%82%E7%9A%84llm%E6%A6%82%E5%BF%B5-1-scaling-law-5f6a409d35c5)<br>

**Papers:**
- [Hestness et al.](https://arxiv.org/abs/1712.00409) 於2017發現在Machine Translation, Language Modeling, Speech Recognition和Image Classification都有出現Scaling law.
- OpenAI [Kaplan et al.2020](https://arxiv.org/abs/2001.08361) 於2020年從計算量、Dataset大小、跟參數量分別討論了Scaling Law。
- [Rosenfeld et al.](https://arxiv.org/abs/2108.07686) 於2021年發表了關於Scaling Law的survey paper。在各種architecture更進一步驗證Scaling Law的普適性。

---
### Chinchilla Scaling Law
**Paper:** [Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556)<br>

如果我們接受原本Scaling Law的定義（模型性能可藉由參數量、Dataset大小、計算量預測），馬上就會衍伸出兩個很重要的問題:<br>

**Return（收益）**： 在固定的訓練計算量之下，我們所能得到的最好性能是多好？<br>
**Allocation（分配）**：我們要怎麼分配我們的模型參數量跟Dataset大小。<br>
（假設計算量 = 參數量 * Dataset size，我們要大模型 * 少量data、中模型 * 中量data、還是小模型 * 大量data）<br>

2022年DeepMind提出Chinchilla Scaling Law，同時解決了這兩個問題，並且依此改善了當時其他大模型的訓練方式。
他們基於三種方式來找到訓練LLM的Scaling Law：<br>
1. 固定模型大小，變化訓練Data數量。
2. 固定計算量（浮點運算），變化模型大小。
3. 對所有實驗結果，直接擬合參數化損失函數。

![](https://miro.medium.com/v2/resize:fit:720/format:webp/1*PWkg8x3Dtr64q-7BI2rrJA.png)
`Method 3 result from Chinchilla Scaling Law，N是模型參數量、D是數據量、其他都是係數`<br>

LLM最終的Loss（Perplexity），會隨著模型放大、數據量變多而下降，並且是跟他們呈現指數映射後線性關係。<br>

Chinchilla最大的貢獻更是在解決Allocation的問題，他們發現<br>
* **數據量（Tokens數）應該要約等於模型參數量的20倍**
* **並且數據量跟模型參數量要同比放大（Ex: 模型放大一倍，數據也要跟著增加一倍）**

---
## Large Language Models

### [Open LLM Leaderboard](https://chat.lmsys.org/?leaderboard)

### Transformer
**Paper:** [Attention Is All You Need](https://arxiv.org/abs/1706.03762)<br>
![](https://miro.medium.com/max/407/1*3pxDWM3c1R_WSW7hVKoaRA.png)

<iframe width="1158" height="652" src="https://www.youtube.com/embed/uhNsUCb2fJI?list=PLJV_el3uVTsPz6CTopeRp2L2t4aL_KgiI" title="【生成式AI導論 2024】第10講：今日的語言模型是如何做文字接龍的 — 淺談Transformer (已經熟悉 Transformer 的同學可略過本講)" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

### ChatGPT
[ChatGPT: Optimizing Language Models for Dialogue](https://openai.com/blog/chatgpt/)<br>
ChatGPT is fine-tuned from a model in the GPT-3.5 series, which finished training in early 2022.<br>

![](https://cdn.openai.com/chatgpt/draft-20221129c/ChatGPT_Diagram.svg)

<iframe width="640" height="455" src="https://www.youtube.com/embed/e0aKI2GGZNg" title="Chat GPT (可能)是怎麼煉成的 - GPT 社會化的過程" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

<iframe width="1152" height="649" src="https://www.youtube.com/embed/cCpErV7To2o?list=PLJV_el3uVTsPz6CTopeRp2L2t4aL_KgiI" title="【生成式AI導論 2024】第6講：大型語言模型修練史 — 第一階段: 自我學習，累積實力 (熟悉機器學習的同學從 15:00 開始看起即可)" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
<iframe width="1152" height="649" src="https://www.youtube.com/embed/Q9cNkUPXUB8?list=PLJV_el3uVTsPz6CTopeRp2L2t4aL_KgiI" title="【生成式AI導論 2024】第7講：大型語言模型修練史 — 第二階段: 名師指點，發揮潛力 (兼談對 ChatGPT 做逆向工程與 LLaMA 時代的開始)" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
<iframe width="1152" height="649" src="https://www.youtube.com/embed/v12IKvF6Cj8?list=PLJV_el3uVTsPz6CTopeRp2L2t4aL_KgiI" title="【生成式AI導論 2024】第8講：大型語言模型修練史 — 第三階段: 參與實戰，打磨技巧 (Reinforcement Learning from Human Feedback, RLHF)" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

---
### [LLaMA](https://huggingface.co/docs/transformers/main/model_doc/llama)
**Paper**: [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)<br>
![](https://miro.medium.com/v2/resize:fit:1100/format:webp/1*nt-ydHhSVsaLXq_HZRaLQA.png)
**Blog**: [Building a Million-Parameter LLM from Scratch Using Python](https://levelup.gitconnected.com/building-a-million-parameter-llm-from-scratch-using-python-f612398f06c2)<br>
**Kaggle**: [LLaMA from scratch](https://www.kaggle.com/rkuo2000/llama-from-scratch/)<br>

---
### GPT4
**Paper**: [GPT-4 Technical Report](https://arxiv.org/abs/2303.08774)<br>
![](https://image-cdn.learnin.tw/bnextmedia/image/album/2023-03/img-1679884936-23656.png?w=1200&output=webp)
**Paper**: [From Sparse to Dense: GPT-4 Summarization with Chain of Density Prompting](https://arxiv.org/abs/2309.04269)<br>
**Blog:** [GPT-4 Code Interpreter: The Next Big Thing in AI](https://medium.com/@aaabulkhair/gpt-4-code-interpreter-the-next-big-thing-in-ai-56bbf72d746)<br>

---
### Falcon-40B
**HuggingFace**: [tiiuae/falcon-40b](https://huggingface.co/tiiuae/falcon-40b)<br>
**Paper**: [The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only](https://arxiv.org/abs/2306.01116)<br>

---
### Vicuna
**HuggingFace**: [lmsys/vicuna-7b-v1.5](https://huggingface.co/lmsys/vicuna-7b-v1.5)<br>
**Paper**: [Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena](https://arxiv.org/abs/2306.05685)<br>
**Code**: [https://github.com/lm-sys/FastChat](https://github.com/lm-sys/FastChat)<br>

---
### [LLaMA-2](https://huggingface.co/meta-llama)
**HuggingFace**: [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf)<br>
**Paper**: [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/abs/2307.09288)<br>
**Code**: [https://github.com/facebookresearch/llama](https://github.com/facebookresearch/llama)<br>

---
### Mistral
**HuggingFace**: [mistralai/Mistral-7B-Instruct-v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)<br>
**Paper**: [Mistral 7B](https://arxiv.org/abs/2310.06825)<br>
**Code**: [https://github.com/mistralai/mistral-src](https://github.com/mistralai/mistral-src)<br>
**Kaggle**: [https://www.kaggle.com/code/rkuo2000/llm-mistral-7b-instruct](https://www.kaggle.com/code/rkuo2000/llm-mistral-7b-instruct)<br>
![](https://www.promptingguide.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fmistral-7B-2.8625353c.png&w=1920&q=75)

---
### Mistral 8X7B
**HuggingFace**: [mistralai/Mixtral-8x7B-v0.1](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1)<br>
**Paper**: [Mixtral of Experts](https://arxiv.org/abs/2401.04088)<br>
![](https://miro.medium.com/v2/resize:fit:720/format:webp/0*91yEJMc_q-QlU-bk.png)

---
### Orca 2
**HuggingFace**: [microsoft/Orca-2-7b](https://huggingface.co/microsoft/Orca-2-7b)<br>
**Paper**: [https://arxiv.org/abs/2311.11045](https://arxiv.org/abs/2311.11045)<br>
**Blog**: [Microsoft's Orca 2 LLM Outperforms Models That Are 10x Larger](https://www.infoq.com/news/2023/12/microsoft-orca-2-llm/)<br>
<p><img src="https://s4.itho.me/sites/default/files/images/1123-Orca-2-microsoft-600.png" width="50%" height="50%"></p>

---
### Taiwan-LLM (優必達+台大)
**HuggingFace**: [yentinglin/Taiwan-LLM-7B-v2.1-chat](https://huggingface.co/yentinglin/Taiwan-LLM-7B-v2.1-chat)<br>
**Paper**: [TAIWAN-LLM: Bridging the Linguistic Divide with a Culturally Aligned Language Model](https://arxiv.org/abs/2311.17487)<br>
**Blog**: [專屬台灣！優必達攜手台大打造「Taiwan LLM」，為何我們需要本土化的AI？](https://www.bnext.com.tw/article/77335/ubitus-ai-taiwan-llm)<br>
**Code**: [https://github.com/MiuLab/Taiwan-LLM](https://github.com/MiuLab/Taiwan-LLM)<br>

---
### Phi-2 (Transformer with 2.7B parameters)
**HuggingFace**: [microsoft/phi-2](https://huggingface.co/microsoft/phi-2)<br>
**Blog**: [Phi-2: The surprising power of small language models](https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/)<br>
**Kaggle:** [https://www.kaggle.com/code/rkuo2000/llm-phi-2](https://www.kaggle.com/code/rkuo2000/llm-phi-2)<br>

---
### [Mamba](https://huggingface.co/collections/Q-bert/mamba-65869481595e25821853d20d)
**HuggingFace**: [Q-bert/Mamba-130M](https://huggingface.co/Q-bert/Mamba-130M)<br>
**Paper**: [Mamba: Linear-Time Sequence Modeling with Selective State Spaces](https://arxiv.org/abs/2312.00752)<br>
![](https://miro.medium.com/v2/resize:fit:4800/format:webp/1*OqdcbRFPuqNd_4hyBcMGHQ.png)

---
### Qwen (通义千问)
**HuggingFace** [Qwen/Qwen1.5-7B-Chat](https://huggingface.co/Qwen/Qwen1.5-7B-Chat)<br>
**Blog**: [Introducing Qwen1.5](https://qwenlm.github.io/blog/qwen1.5/)<br>
**Code**: [https://github.com/QwenLM/Qwen1.5](https://github.com/QwenLM/Qwen1.5)<br>

---
### Yi (零一万物)
**HuggingFace**: [01-ai/Yi-6B-Chat](https://huggingface.co/01-ai/Yi-6B-Chat)<br>
**Paper**: [CMMMU: A Chinese Massive Multi-discipline Multimodal Understanding Benchmark](https://arxiv.org/abs/2401.11944)<br>
**Paper**: [Yi: Open Foundation Models by 01.AI](https://arxiv.org/abs/2403.04652)<br>

---
### Orca-Math
**Paper**: [Orca-Math: Unlocking the potential of SLMs in Grade School Math](https://arxiv.org/abs/2402.14830)<br>
**HuggingFace**: [https://huggingface.co/datasets/microsoft/orca-math-word-problems-200k](https://huggingface.co/datasets/microsoft/orca-math-word-problems-200k)<br>

---
### [BitNet](https://github.com/microsoft/BitNet)
**Paper**: [BitNet: Scaling 1-bit Transformers for Large Language Models](https://arxiv.org/abs/2310.11453)<br>
**Paper**: [The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits](https://arxiv.org/abs/2402.17764)<br>
bitnet.cpp is the official inference framework for 1-bit LLMs (e.g., BitNet b1.58). 
![](https://github.com/microsoft/BitNet/raw/main/assets/intel_performance.jpg)

---
### Gemma
**HuggingFace**: [google/gemma-1.1-7b-it](https://huggingface.co/google/gemma-1.1-7b-it)<br>
**Blog**: [Gemma: Introducing new state-of-the-art open models](https://blog.google/technology/developers/gemma-open-models/)<br>
**Kaggle**: [https://www.kaggle.com/code/nilaychauhan/fine-tune-gemma-models-in-keras-using-lora](https://www.kaggle.com/code/nilaychauhan/fine-tune-gemma-models-in-keras-using-lora)<br>

---
### [Gemini-1.5](https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/)

---
### [Claude 3](https://www.anthropic.com/news/claude-3-family)
![](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F9ad98d612086fe52b3042f9183414669b4d2a3da-2200x1954.png&w=3840&q=75)

---
### Breeze (達哥)
**HuggingFace**: [MediaTek-Research/Breeze-7B-Instruct-v0_1](https://huggingface.co/MediaTek-Research/Breeze-7B-Instruct-v0_1)<br>
**Paper**: [Breeze-7B Technical Report](https://arxiv.org/abs/2403.02712)<br>
**Blog**: [Breeze-7B: 透過 Mistral-7B Fine-Tune 出來的繁中開源模型](https://blog.infuseai.io/quick-demo-3-breeze-7b-mediatek-intro-3e2f8e2f6da9)<br>

---
### Bialong (白龍)
**HuggingFace**: [INX-TEXT/Bailong-instruct-7B](https://huggingface.co/INX-TEXT/Bailong-instruct-7B)<br>
**Paper**: [Bailong: Bilingual Transfer Learning based on QLoRA and Zip-tie Embedding](https://arxiv.org/abs/2404.00862)<br>

---
### [TAIDE](https://taide.tw/index)
**HuggingFace**: [taide/TAIDE-LX-7B-Chat](https://huggingface.co/taide/TAIDE-LX-7B-Chat)<br>
* TAIDE-LX-7B: 以 LLaMA2-7b 為基礎，僅使用繁體中文資料預訓練 (continuous pretraining)的模型，適合使用者會對模型進一步微調(fine tune)的使用情境。因預訓練模型沒有經過微調和偏好對齊，可能會產生惡意或不安全的輸出，使用時請小心。
* TAIDE-LX-7B-Chat: 以 TAIDE-LX-7B 為基礎，透過指令微調(instruction tuning)強化辦公室常用任務和多輪問答對話能力，適合聊天對話或任務協助的使用情境。TAIDE-LX-7B-Chat另外有提供4 bit 量化模型，量化模型主要是提供使用者的便利性，可能會影響效能與更多不可預期的問題，還請使用者理解與注意。

---
### [Llama-3](https://ai.meta.com/blog/meta-llama-3/)
**HuggingFace**: [meta-llama/Meta-Llama-3-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct)<br>
**Code**: [https://github.com/meta-llama/llama3/](https://github.com/meta-llama/llama3/)<br>
![](https://scontent.ftpe3-2.fna.fbcdn.net/v/t39.2365-6/438922663_1135166371264105_805978695964769385_n.png?_nc_cat=107&ccb=1-7&_nc_sid=e280be&_nc_ohc=VlupGTPFG1UAb6vzA2n&_nc_ht=scontent.ftpe3-2.fna&oh=00_AfCRhEYLtA4OYbvEyNCRXBdU9riWMtwtBWnk79O-SIigbg&oe=663C005E)

---
### Phi-3
**HuggingFace**: [microsoft/Phi-3-mini-4k-instruct"](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct)<br>
**Blog**: [Introducing Phi-3: Redefining what’s possible with SLMs](https://azure.microsoft.com/en-us/blog/introducing-phi-3-redefining-whats-possible-with-slms/)<br>

---
### Octopus v4
**HuggingFace**: [NexaAIDev/Octopus-v4](https://huggingface.co/NexaAIDev/Octopus-v4)<br>
**Paper:** [Octopus v4: Graph of language models](https://arxiv.org/abs/2404.19296)<br>
**Code:** [https://github.com/NexaAI/octopus-v4](https://github.com/NexaAI/octopus-v4)<br>
[design demo](https://graph.nexa4ai.com/)<br>

---
### [Llama 3.1](https://ai.meta.com/blog/meta-llama-3-1/)
**HuggingFace**: [meta-llama/Meta-Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct)<br>
![](https://scontent.ftpe3-1.fna.fbcdn.net/v/t39.2365-6/452673884_1646111879501055_1352920258421649752_n.png?_nc_cat=100&ccb=1-7&_nc_sid=e280be&_nc_ohc=tHtG_dev9lcQ7kNvgFirAn9&_nc_ht=scontent.ftpe3-1.fna&oh=00_AYDvpGKrjgZlN5eVxo7ppCqw2Umq03ytaednlW3mTpCDvQ&oe=66C45828)

---
### Grok-2
Grok-2 & Grok-2 mini, achieve performance levels competitive to other frontier models in areas such as graduate-level science knowledge (GPQA), general knowledge (MMLU, MMLU-Pro), and math competition problems (MATH). Additionally, Grok-2 excels in vision-based tasks, delivering state-of-the-art performance in visual math reasoning (MathVista) and in document-based question answering (DocVQA).

---
### Phi-3.5
**HuggingFace**: [microsoft/Phi-3.5-mini-instruct](https://huggingface.co/microsoft/Phi-3.5-mini-instruct)<br>
**HuggingFace**: [microsoft/Phi-3.5-vision-instruct](https://huggingface.co/microsoft/Phi-3.5-vision-instruct)<br>
**HuggingFace**: [microsoft/Phi-3.5-MoE-instruct](https://huggingface.co/microsoft/Phi-3.5-MoE-instruct)<br>
**News**: [Microsoft Unveils Phi-3.5: Powerful AI Models Punch Above Their Weight](https://www.maginative.com/article/microsoft-unveils-phi-3-5-powerful-ai-models-punch-above-their-weight/)<br>
![](https://www.maginative.com/content/images/size/w1600/2024/08/Screenshot-2024-08-21-at-2.11.41-AM.png)
![](https://www.maginative.com/content/images/size/w1600/2024/08/Screenshot-2024-08-21-at-2.09.00-AM.png)

---
### [OpenAI o1](https://openai.com/o1/)
**Blog**: [Introducing OpenAI o1-preview](https://openai.com/index/introducing-openai-o1-preview/)<br>
![](https://i0.wp.com/claire-chang.com/wp-content/uploads/2024/09/6-2-1.jpg?resize=1024%2C390&ssl=1)

---
### Qwen2.5 
**HuggingFace**: [Qwen/Qwen2.5-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct)<br>
* Qwen2.5: 0.5B, 1.5B, 3B, 7B, 14B, 32B, 72B
* Qwen2.5-Coder: 1.5B, 7B, coming 32B
* Qwen2.5-Math: 1.5B, 7B, 72B
  
**Blog**: [阿里雲AI算力大升級！發佈100個開源Qwen 2.5模型及視頻AI模型](https://itpromag.com/2024/09/20/alicloud-qwen/)<br>
![](https://itpromag.com/wp-content/uploads/2024/09/Qwen2.5-Max-1024x680.jpg)

---
### [NVLM 1.0](https://nvlm-project.github.io/)
**Paper**: [NVLM: Open Frontier-Class Multimodal LLMs](https://arxiv.org/abs/2409.11402)<br>
![](https://nvlm-project.github.io/overview-v7.png)

---
### [Llama 3.2](https://www.llama.com/)
**Blog**: [Llama 3.2: Revolutionizing edge AI and vision with open, customizable models](https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/)<br>
**HuggingFace**: [meta-llama/Llama-3.2-1B-Instruct](https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct)<br>
**HuggingFace**: [meta-llama/Llama-3.2-3B-Instruct](https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct)<br>
**HuggingFace**: [meta-llama/Llama-3.2-11B-Vision-Instruct](https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct)<br>
![](https://scontent.ftpe3-2.fna.fbcdn.net/v/t39.2365-6/461288018_1255239495501495_271827633811450582_n.png?_nc_cat=102&ccb=1-7&_nc_sid=e280be&_nc_ohc=mIQhr3Vn3ugQ7kNvgHVEGc8&_nc_zt=14&_nc_ht=scontent.ftpe3-2.fna&_nc_gid=AUEcRDCdgG7eJD2WoQ5ORR4&oh=00_AYCl8rFbEO_Skop4igq0oQXwce5l3jglmYKzArlICpTzyA&oe=67304A6D)

---
### [LFM Liquid-3B](https://www.liquid.ai/liquid-foundation-models)
![](https://cdn.prod.website-files.com/6557a2b6957fcb7aeb0efcf0/66f6f79029bbf7e233406c45_Desktop.avif)
**[Try Liquid](https://playground.liquid.ai/chat?model=cm1ooqdqo000208jx67z86ftk)** <br>

---
### Llama 3.3
**HuggingFace**:  [meta-llama/Llama-3.3-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct)<br>
**Blog**: [Meta公布輕巧版多語言模型Llama 3.3](https://www.ithome.com.tw/news/166407)<br>

---
### [OpenAI o3-mini](https://openai.com/index/openai-o3-mini/)
![](https://images.ctfassets.net/kftzwdyauwt9/8P5ddf0mQCNmstUbSceyU/53aacead98e5224b73fd422487c87030/Competition_Math.png?w=1920&q=80&fm=webp)

---
### DeepSeek-R1
**Paper**: [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://arxiv.org/abs/2501.12948)<br>
**Code**: [https://github.com/deepseek-ai/DeepSeek-R1](https://github.com/deepseek-ai/DeepSeek-R1)<br>
![](https://github.com/deepseek-ai/DeepSeek-R1/raw/main/figures/benchmark.jpg)

---
### Llama-Breeze2
**HuggingFace**: [MediaTek-Research/Llama-Breeze2-8B-Instruct](https://huggingface.co/MediaTek-Research/Llama-Breeze2-8B-Instruct)<br>
**HuggingFace**: [MediaTek-Research/Llama-Breeze2-3B-Instruct](https://huggingface.co/MediaTek-Research/Llama-Breeze2-3B-Instruct)<br>
**Paper**: [The Breeze 2 Herd of Models: Traditional Chinese LLMs Based on Llama with Vision-Aware and Function-Calling Capabilities
](https://arxiv.org/abs/2501.13921)<br>
**Blog**: [聯發科一口氣開源2款繁中多模態小模型、符合臺灣口音的語音合成模型](https://www.ithome.com.tw/news/167427)<br>
**Blog**: [如何讓模型更懂繁中知識？聯發科研究團隊揭技術關鍵](https://www.ithome.com.tw/news/167459)<br>
![](https://s4.itho.me/sites/default/files/styles/picture_size_large/public/field/image/ying_mu_xie_qu_hua_mian_2025-02-19_193153.png?itok=ia39V76q)

---
### [Grok-3 The Age of Reasoning Agents](https://x.ai/news/grok-3)
![](https://s4.itho.me/sites/default/files/images/0219-xAI-Grok-Benchmark.png)
![](https://s4.itho.me/sites/default/files/images/0219-xAI-Grok-Benchmark-AIME%202025.png)

---
### [Phi-4-multimodal](https://www.ithome.com.tw/news/167591)
Phi-4-multimodal具有56億參數，支援12.8萬Token的上下文長度，並透過監督式微調、直接偏好最佳化（DPO）與人類回饋強化學習（RLHF）等方式，提升指令遵循能力與安全性。在語言支援方面，文字處理涵蓋超過20種語言，包括中文、日文、韓文、德文與法文等，語音處理則涵蓋英語、中文、西班牙語、日語等主要語種，圖像處理目前則以英文為主。<br>
**GuggingFace**: [microsoft/Phi-4-multimodal-instruct](https://huggingface.co/microsoft/Phi-4-multimodal-instruct)<br>
![](https://huggingface.co/microsoft/Phi-4-multimodal-instruct/resolve/main/figures/speech_recognition.png)

---
### [Gemini-2.5](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)

---
### [Llama-4](https://ai.meta.com/blog/llama-4-multimodal-intelligence/)
**Blog**: [Implementing LLaMA 4 from Scratch](https://www.dailydoseofds.com/building-llama-4-from-scratch-with-python/)<br>
![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0681d04f-0cd6-45a7-b1f1-a37f9269d01d_1116x1126.gif)

**Kaggle**: [https://www.kaggle.com/code/rkuo2000/llama4-from-scratch](https://www.kaggle.com/code/rkuo2000/llama4-from-scratch)<br>

---
### [Grok-4](https://x.ai/news/grok-4)

---
### [GPT-5](https://openai.com/zh-Hant/index/introducing-gpt-5/)

---
### [Gemini-2.5 Family](https://blog.google/products/gemini/gemini-2-5-model-family-expands/)

---
### [Qwen3-Next](Qwen3-Next：迈向更极致的训练推理性价比)
**HuggineFace**: 
* [Qwen/Qwen3-Next-80B-A3B-Instruct](https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Instruct)
* [Qwen/Qwen3-Next-80B-A3B-Thinking](https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Thinking)
  
![](https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-Next/archtecture.png)

---
### [Qwen3-Omni](https://arxiv.org/html/2509.17765v1)
**Paper**: [Qwen3-Omni Technical Report](https://arxiv.org/html/2509.17765v1)<br>
![](https://arxiv.org/html/2509.17765v1/figures/overview.png)

---
### [Olmo3](https://allenai.org/blog/olmo3)
**Blog**: [Ai2釋出真開源思考模型Olmo 3，支援可回溯推理與長上下文](https://www.ithome.com.tw/news/172441)<br>

---
### [Claude Opus 4.5](https://www.anthropic.com/news/claude-opus-4-5)
![](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F7022a87aeb6eab1458d68412bc927306224ea9eb-3840x2160.png&w=3840&q=75)

---
## safe AI

### Constitutional AI
**Paper**: [Constitutional AI: Harmlessness from AI Feedback](https://arxiv.org/abs/2212.08073)
Two key phases:<br>
1. Supervised Learning Phase (SL Phase)
- Step1: The learning starts using the samples from the initial model
- Step2: From these samples, the model generates self-critiques and revisions
- Step3: Fine-tine the original model with these revisions
2. Reinforcement Learning Phase (RL Phease)
- Step1. The model uses samples from the fine-tuned model.
- Step2. Use a model to compare the outputs from samples from the initial model and the fine-tuned model
- Step3. Decide which sample is better. (RLHF)
- Step4. Train a new "preference model" from the new dataset of AI preferences.
This new "prefernece model" will then be used to re-train the RL (as a reward signal).
It is now the RLHAF (Reinforcement Learning from AI feedback)

---
### Attack LLM
**Blog**: [如何攻擊 LLM (ChatGPT) ?](https://vocus.cc/article/65715fa2fd89780001fc7967)
* JailBreak
* Prompt Injection
* Data poisoning

---
## LLM running locally

### [LM Studio](https://lmstudio.ai/)
![](https://github.com/rkuo2000/AI-course/blob/main/assets/images/LM_studio_server.png?raw=true)

![](https://github.com/rkuo2000/AI-course/blob/main/assets/images/LM_studio_client.png?raw=true)

---
### [Ollama](https://ollama.com/download)
**Code**: [Code](https://github.com/ollama/ollama)<br>

**Kaggle**: <br>
* [Langchain RAG](https://www.kaggle.com/code/rkuo2000/langchain-rag)<br>
* [langchain-python-rag-privategpt](https://github.com/ollama/ollama/tree/main/examples/langchain-python-rag-privategpt)<br>

---
### [Jan - Local AI Assistant](https://jan.ai/)
**Code**: [https://github.com/menloresearch/jan](https://github.com/menloresearch/jan)<br>

---
### PrivateGPT
**Code**: [https://github.com/zylon-ai/private-gpt](https://github.com/zylon-ai/private-gpt)<br>
![](https://github.com/zylon-ai/private-gpt/raw/main/fern/docs/assets/ui.png?raw=true)

---
### [vLLM](https://docs.vllm.ai/en/latest/index.html)
**Code**: [https://github.com/vllm-project/vllm](https://github.com/vllm-project/vllm)<br>
`pip install vllm`<br>

---
### [llama.cpp](https://github.com/ggml-org/llama.cpp)
LLM inference in C/C++<br>

---
## RLM
**Paper**: [Reasoning Language Models: A Blueprint](https://arxiv.org/pdf/2501.11223)<br>

### [LLM Reasoning](https://dennyzhou.github.io/LLM-Reasoning-Stanford-CS-25.pdf)
<iframe width="1028" height="578" src="https://www.youtube.com/embed/s00fy5RkCHc" title="【人工智能】推理的本质 | Denny Zhou斯坦福讲座 | 什么是推理 | 思考的必要性 | 思维链提示 | 贪婪解码 | 置信度 | step-by-step | SFT | 验证器" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

---
### Chain-of-Thought Prompting
**Paper**: [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)<br>
![](https://ar5iv.labs.arxiv.org/html/2201.11903/assets/x1.png)

---
### [ReAct Prompting](https://react-lm.github.io/)
**Paper**: [ReAct: Synergizing Reasoning and Acting in Language Models](https://arxiv.org/abs/2210.03629)<br>
**Code**: [https://github.com/ysymyth/ReAct](https://github.com/ysymyth/ReAct)<br>
![](https://react-lm.github.io/files/diagram.png)

---
### Tree-of-Thoughts
**Paper**: [Tree of Thoughts: Deliberate Problem Solving with Large Language Models](https://arxiv.org/abs/2305.10601)<br>
**Code**: [https://github.com/princeton-nlp/tree-of-thought-llm](https://github.com/princeton-nlp/tree-of-thought-llm)<br>
**Code**: [https://github.com/kyegomez/tree-of-thoughts](https://github.com/kyegomez/tree-of-thoughts)<br>
![](https://github.com/princeton-nlp/tree-of-thought-llm/blob/master/pics/teaser.png?raw=true)

---
### Reinforcement Pre-Training
**Paper**: [Reinforcement Pre-Training](https://arxiv.org/html/2506.08007v1)
[Microsoft and China AI Research Possible Reinforcement Pre-Training Breakthrough](https://www.nextbigfuture.com/2025/06/microsoft-and-china-ai-research-possible-reinforcement-pre-training-breakthrough.html)<br>
![](https://nextbigfuture.s3.amazonaws.com/uploads/2025/06/Screenshot-2025-06-10-at-5.10.38-PM.jpg)

---
### Teaching LLMs to Plan
**Paper**: [Teaching LLMs to Plan: Logical Chain-of-Thought Instruction Tuning for Symbolic Planning](https://arxiv.org/abs/2509.13351)<br>

---
### [Alpaca-CoT](https://github.com/PhoebusSi/Alpaca-CoT)
Alpaca-CoT: An Instruction-Tuning Platform with Unified Interface for Instruction Collection, Parameter-efficient Methods, and Large Language Models<br>
![](https://github.com/PhoebusSi/Alpaca-CoT/raw/main/figures/platform-en.png)

---
### TRM (Tiny Recursive Model)
**Paper**: [Less is More: Recursive Reasoning with Tiny Networks](https://arxiv.org/abs/2510.04871)<br>
**Code**: [https://github.com/SamsungSAILMontreal/TinyRecursiveModels](https://github.com/SamsungSAILMontreal/TinyRecursiveModels)<br>
![](https://camo.githubusercontent.com/1d7c2b73b2e597b989dee0b4950bc997cce2a21769c83bab701b7cd6b0796c67/68747470733a2f2f416c657869614a4d2e6769746875622e696f2f6173736574732f696d616765732f54524d5f6669672e706e67)

---
## Prompt Engineering

### Perfect Prompt Structure
![](https://ai-rockstars.com/wp-content/uploads/2025/04/prompt-engineering-perfect-prompt-structure.png)

---
### 訓練不了人工智慧？你可以訓練你自己
<iframe width="781" height="439" src="https://www.youtube.com/embed/FetyDBqAZ24" title="訓練不了人工智慧？你可以訓練你自己 | 研究生精進計畫" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

---
### Thinking Claude
[17歲高中生寫出「神級Prompt」強化Claude推理能力媲美o1模型，如何實現？](https://www.blocktempo.com/17-year-old-high-schooler-creates-a-prompt-how-to-use-gemini-to-copy-paste-and-develop-games/)

### Thinking Gemini
[https://github.com/lanesky/thinking-gemini](https://github.com/lanesky/thinking-gemini)<br>
<iframe width="781" height="439" src="https://www.youtube.com/embed/GkkWtA6MLXs" title="Thinking Gemini| 复制17岁高中生写的神级Prompt到Gemini" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

---
## Context Engineering

### [什麼是 Context Engineering 上下文工程?](https://ihower.tw/blog/12817-context-engineering)<br>
![](https://ihower.tw/blog/wp-content/uploads/2025/07/image-1-1024x849.png)

---
### [A Survey of Context Engineering for Large Language Models](https://arxiv.org/html/2507.13334v2)
![](https://arxiv.org/html/2507.13334v2/x3.png)

---
### [情境工程（Context Engineering）解析：打造實用 AI Agent 的關鍵技巧，與提示工程（Prompt Engineering）有什麼不同？](https://ikala.ai/zh-tw/blog/ikala-ai-insight/introduction-to-context-engineering-ai-agent-vs-prompt-engineering/)<br>
![](https://github.com/rkuo2000/AI-course/blob/main/assets/images/LLM_Context_vs_Prompt.png?raw=true)

---
### ACE-open
**Paper**: [Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models](https://www.arxiv.org/abs/2510.04618)<br>
**Code**: [https://github.com/sci-m-wang/ACE-open](https://github.com/sci-m-wang/ACE-open)<br>
![](https://arxiv.org/html/2510.04618v1/x3.png)

<br>
<br>

*This site was last updated {{ site.time | date: "%B %d, %Y" }}.*
